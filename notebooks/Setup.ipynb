{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to the COVID-19 Open Research Dataset\n",
        "\n",
        "The COVID-19 Open Research Dataset (CORD-19) is a collection of over 50,000 scholarly articles - including over 40,000 with full text - about COVID-19, SARS-CoV-2, and related coronaviruses. This dataset has been made freely available with the goal to aid research communities combat the COVID-19 pandemic. It has been made available by the Allen Institute for AI in partnership with leading research groups to prepare and distribute the COVID-19 Open Research Dataset (CORD-19), in response to the COVID-19 pandemic.\n",
        "\n",
        "During this lab you will learn how to process and analyze a subset of the articles present in the dataset, group them together into a series of clusters, and use Automated ML to train a machine learning model capable of classifying new articles as they are published."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n",
        "\n",
        "We will start off by installing a few packages, such as `nltk` for text processing and `wordcloud`, `seaborn`, and `yellowbrick` for various visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim==3.8.2 in c:\\users\\photo\\appdata\\roaming\\python\\python310\\site-packages (3.8.2)\n",
            "Requirement already satisfied: six>=1.5.0 in c:\\users\\photo\\miniconda3\\lib\\site-packages (from gensim==3.8.2) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\photo\\appdata\\roaming\\python\\python310\\site-packages (from gensim==3.8.2) (1.23.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\photo\\miniconda3\\lib\\site-packages (from gensim==3.8.2) (6.3.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\photo\\miniconda3\\lib\\site-packages (from gensim==3.8.2) (1.10.1)\n"
          ]
        }
      ],
      "source": [
        "# !pip install nltk\n",
        "# !pip install wordcloud\n",
        "# !pip install seaborn\n",
        "# !pip install yellowbrick\n",
        "!pip install  gensim==3.8.2\n",
        "# !pip install azureml-core \n",
        "# !pip install azureml-widgets "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll first download stopwords and the Punkt tokenizer models present in the `nltk` package, in order to be able to process the articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "gather": {
          "logged": 1676793367486
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\photo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\photo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll also import the rest of the modules needed in this notebook, and do a quick sanity-check on the Azure ML SDK version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "gather": {
          "logged": 1676793371262
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Azure ML SDK Version:  1.49.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from string import punctuation\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set_palette('Set2')\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans, SpectralClustering, DBSCAN, Birch, AgglomerativeClustering\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.stem import SnowballStemmer, PorterStemmer\n",
        "\n",
        "from azureml.core import Workspace, Datastore, Dataset, VERSION\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core import Dataset, Workspace, Experiment\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.train.automl.run import AutoMLRun\n",
        "from azureml.widgets import RunDetails\n",
        "from azureml.automl.core.featurization.featurizationconfig import FeaturizationConfig\n",
        "\n",
        "print(\"Azure ML SDK Version: \", VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the Covid-19 data\n",
        "\n",
        "CORD-19 has been uploaded as an Azure Open Dataset, we will connect to it and use it's API to download the dataset locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "gather": {
          "logged": 1676793381332
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You already Downloaded teh metadata.csv file. We will use that one.\n"
          ]
        }
      ],
      "source": [
        "baseUrl = \"https://stgai2023.blob.core.windows.net/opendata/\"\n",
        "import requests\n",
        "if not os.path.exists(\"metadata.csv\"):\n",
        "    print(\"Hold on as I pull 1.5Gigs of data...It'll be worth it!\")\n",
        "    response = requests.request(\"GET\", baseUrl+\"metadata.csv\")\n",
        "    csvData = response.text\n",
        "    file = open('./metadata.csv', 'w')\n",
        "    file.write(csvData)\n",
        "    file.close()\n",
        "    print(\"Got it. Woah. That's some data!\")\n",
        "else:\n",
        "    print(\"You already Downloaded teh metadata.csv file. We will use that one.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display a sample of the dataset (top 5 rows)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "gather": {
          "logged": 1676793413126
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\photo\\AppData\\Local\\Temp\\ipykernel_20428\\1400788635.py:1: DtypeWarning: Columns (1,4,5,6,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  metadata = pd.read_csv(\"./metadata.csv\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cord_uid</th>\n",
              "      <th>sha</th>\n",
              "      <th>source_x</th>\n",
              "      <th>title</th>\n",
              "      <th>doi</th>\n",
              "      <th>pmcid</th>\n",
              "      <th>pubmed_id</th>\n",
              "      <th>license</th>\n",
              "      <th>abstract</th>\n",
              "      <th>publish_time</th>\n",
              "      <th>authors</th>\n",
              "      <th>journal</th>\n",
              "      <th>mag_id</th>\n",
              "      <th>who_covidence_id</th>\n",
              "      <th>arxiv_id</th>\n",
              "      <th>pdf_json_files</th>\n",
              "      <th>pmc_json_files</th>\n",
              "      <th>url</th>\n",
              "      <th>s2_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ug7v899j</td>\n",
              "      <td>d1aafb70c066a2068b02786f8929fd9c900897fb</td>\n",
              "      <td>PMC</td>\n",
              "      <td>Clinical features of culture-proven Mycoplasma...</td>\n",
              "      <td>10.1186/1471-2334-1-6</td>\n",
              "      <td>PMC35282</td>\n",
              "      <td>11472636</td>\n",
              "      <td>no-cc</td>\n",
              "      <td>OBJECTIVE: This retrospective chart review des...</td>\n",
              "      <td>2001-07-04</td>\n",
              "      <td>Madani, Tariq A; Al-Ghamdi, Aisha A</td>\n",
              "      <td>BMC Infect Dis</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>document_parses/pdf_json/d1aafb70c066a2068b027...</td>\n",
              "      <td>document_parses/pmc_json/PMC35282.xml.json</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>02tnwd4m</td>\n",
              "      <td>6b0567729c2143a66d737eb0a2f63f2dce2e5a7d</td>\n",
              "      <td>PMC</td>\n",
              "      <td>Nitric oxide: a pro-inflammatory mediator in l...</td>\n",
              "      <td>10.1186/rr14</td>\n",
              "      <td>PMC59543</td>\n",
              "      <td>11667967</td>\n",
              "      <td>no-cc</td>\n",
              "      <td>Inflammatory diseases of the respiratory tract...</td>\n",
              "      <td>2000-08-15</td>\n",
              "      <td>Vliet, Albert van der; Eiserich, Jason P; Cros...</td>\n",
              "      <td>Respir Res</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>document_parses/pdf_json/6b0567729c2143a66d737...</td>\n",
              "      <td>document_parses/pmc_json/PMC59543.xml.json</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ejv2xln0</td>\n",
              "      <td>06ced00a5fc04215949aa72528f2eeaae1d58927</td>\n",
              "      <td>PMC</td>\n",
              "      <td>Surfactant protein-D and pulmonary host defense</td>\n",
              "      <td>10.1186/rr19</td>\n",
              "      <td>PMC59549</td>\n",
              "      <td>11667972</td>\n",
              "      <td>no-cc</td>\n",
              "      <td>Surfactant protein-D (SP-D) participates in th...</td>\n",
              "      <td>2000-08-25</td>\n",
              "      <td>Crouch, Erika C</td>\n",
              "      <td>Respir Res</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>document_parses/pdf_json/06ced00a5fc04215949aa...</td>\n",
              "      <td>document_parses/pmc_json/PMC59549.xml.json</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2b73a28n</td>\n",
              "      <td>348055649b6b8cf2b9a376498df9bf41f7123605</td>\n",
              "      <td>PMC</td>\n",
              "      <td>Role of endothelin-1 in lung disease</td>\n",
              "      <td>10.1186/rr44</td>\n",
              "      <td>PMC59574</td>\n",
              "      <td>11686871</td>\n",
              "      <td>no-cc</td>\n",
              "      <td>Endothelin-1 (ET-1) is a 21 amino acid peptide...</td>\n",
              "      <td>2001-02-22</td>\n",
              "      <td>Fagan, Karen A; McMurtry, Ivan F; Rodman, David M</td>\n",
              "      <td>Respir Res</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>document_parses/pdf_json/348055649b6b8cf2b9a37...</td>\n",
              "      <td>document_parses/pmc_json/PMC59574.xml.json</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9785vg6d</td>\n",
              "      <td>5f48792a5fa08bed9f56016f4981ae2ca6031b32</td>\n",
              "      <td>PMC</td>\n",
              "      <td>Gene expression in epithelial cells in respons...</td>\n",
              "      <td>10.1186/rr61</td>\n",
              "      <td>PMC59580</td>\n",
              "      <td>11686888</td>\n",
              "      <td>no-cc</td>\n",
              "      <td>Respiratory syncytial virus (RSV) and pneumoni...</td>\n",
              "      <td>2001-05-11</td>\n",
              "      <td>Domachowske, Joseph B; Bonville, Cynthia A; Ro...</td>\n",
              "      <td>Respir Res</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>document_parses/pdf_json/5f48792a5fa08bed9f560...</td>\n",
              "      <td>document_parses/pmc_json/PMC59580.xml.json</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cord_uid                                       sha source_x  \\\n",
              "0  ug7v899j  d1aafb70c066a2068b02786f8929fd9c900897fb      PMC   \n",
              "1  02tnwd4m  6b0567729c2143a66d737eb0a2f63f2dce2e5a7d      PMC   \n",
              "2  ejv2xln0  06ced00a5fc04215949aa72528f2eeaae1d58927      PMC   \n",
              "3  2b73a28n  348055649b6b8cf2b9a376498df9bf41f7123605      PMC   \n",
              "4  9785vg6d  5f48792a5fa08bed9f56016f4981ae2ca6031b32      PMC   \n",
              "\n",
              "                                               title                    doi  \\\n",
              "0  Clinical features of culture-proven Mycoplasma...  10.1186/1471-2334-1-6   \n",
              "1  Nitric oxide: a pro-inflammatory mediator in l...           10.1186/rr14   \n",
              "2    Surfactant protein-D and pulmonary host defense           10.1186/rr19   \n",
              "3               Role of endothelin-1 in lung disease           10.1186/rr44   \n",
              "4  Gene expression in epithelial cells in respons...           10.1186/rr61   \n",
              "\n",
              "      pmcid pubmed_id license  \\\n",
              "0  PMC35282  11472636   no-cc   \n",
              "1  PMC59543  11667967   no-cc   \n",
              "2  PMC59549  11667972   no-cc   \n",
              "3  PMC59574  11686871   no-cc   \n",
              "4  PMC59580  11686888   no-cc   \n",
              "\n",
              "                                            abstract publish_time  \\\n",
              "0  OBJECTIVE: This retrospective chart review des...   2001-07-04   \n",
              "1  Inflammatory diseases of the respiratory tract...   2000-08-15   \n",
              "2  Surfactant protein-D (SP-D) participates in th...   2000-08-25   \n",
              "3  Endothelin-1 (ET-1) is a 21 amino acid peptide...   2001-02-22   \n",
              "4  Respiratory syncytial virus (RSV) and pneumoni...   2001-05-11   \n",
              "\n",
              "                                             authors         journal  mag_id  \\\n",
              "0                Madani, Tariq A; Al-Ghamdi, Aisha A  BMC Infect Dis     NaN   \n",
              "1  Vliet, Albert van der; Eiserich, Jason P; Cros...      Respir Res     NaN   \n",
              "2                                    Crouch, Erika C      Respir Res     NaN   \n",
              "3  Fagan, Karen A; McMurtry, Ivan F; Rodman, David M      Respir Res     NaN   \n",
              "4  Domachowske, Joseph B; Bonville, Cynthia A; Ro...      Respir Res     NaN   \n",
              "\n",
              "  who_covidence_id arxiv_id  \\\n",
              "0              NaN      NaN   \n",
              "1              NaN      NaN   \n",
              "2              NaN      NaN   \n",
              "3              NaN      NaN   \n",
              "4              NaN      NaN   \n",
              "\n",
              "                                      pdf_json_files  \\\n",
              "0  document_parses/pdf_json/d1aafb70c066a2068b027...   \n",
              "1  document_parses/pdf_json/6b0567729c2143a66d737...   \n",
              "2  document_parses/pdf_json/06ced00a5fc04215949aa...   \n",
              "3  document_parses/pdf_json/348055649b6b8cf2b9a37...   \n",
              "4  document_parses/pdf_json/5f48792a5fa08bed9f560...   \n",
              "\n",
              "                               pmc_json_files  \\\n",
              "0  document_parses/pmc_json/PMC35282.xml.json   \n",
              "1  document_parses/pmc_json/PMC59543.xml.json   \n",
              "2  document_parses/pmc_json/PMC59549.xml.json   \n",
              "3  document_parses/pmc_json/PMC59574.xml.json   \n",
              "4  document_parses/pmc_json/PMC59580.xml.json   \n",
              "\n",
              "                                                 url  s2_id  \n",
              "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...    NaN  \n",
              "1  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
              "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
              "3  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
              "4  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  "
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metadata = pd.read_csv(\"./metadata.csv\")\n",
        "metadata.head(5) # let's see the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some of the articles do not have any associated documents, so we will filter those out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "gather": {
          "logged": 1676793413600
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset contains 1056660 entries, out of which 373766 have associated json documents\n"
          ]
        }
      ],
      "source": [
        "metadata_with_docs = metadata[metadata['pdf_json_files'].isna() == False]\n",
        "\n",
        "print(f'Dataset contains {metadata.shape[0]} entries, out of which {metadata_with_docs.shape[0]} have associated json documents')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the percentage of items in the dataset that have associated JSON documents (research papers that have extra metadata associated with them)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "gather": {
          "logged": 1676793413942
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document local filepath: https://stgai2023.blob.core.windows.net/opendata/document_parses/pdf_json/d1aafb70c066a2068b02786f8929fd9c900897fb.json\n"
          ]
        }
      ],
      "source": [
        "# Change the document index in order to preview a different article\n",
        "DOCUMENT_INDEX = 0 \n",
        "example_entry = metadata_with_docs.iloc[DOCUMENT_INDEX]\n",
        "jsonDatauri = baseUrl+example_entry[\"pdf_json_files\"]\n",
        "\n",
        "# filepath = os.path.join(covid_dirpath, example_entry['pdf_json_files'])\n",
        "print(f'Document local filepath: {filepath}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will display the list of elements that are available for the selected document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1676793415974
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data elements: paper_id, metadata, abstract, body_text, bib_entries, ref_entries, back_matter\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "response = requests.request(\"GET\", jsonDatauri)\n",
        "data = response.json()\n",
        "        \n",
        "print(f'Data elements: { \", \".join(data.keys())}' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "View the full text version of the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "gather": {
          "logged": 1676793495625
        }
      },
      "outputs": [],
      "source": [
        "stop_tokens = nltk.corpus.stopwords.words('english') + list(punctuation) + ['et', 'al.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "gather": {
          "logged": 1676793499553
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "covid_dirpath = \"./metadata.csv\"\n",
        "\n",
        "class Reader:\n",
        "    \"\"\"Class used to read the files associated with an article\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.stemmer = SnowballStemmer('english')\n",
        "    \n",
        "    def read_file_to_json(self, filepath):\n",
        "        response = requests.request(\"GET\", jsonDatauri)\n",
        "        data = response.json()        \n",
        "        return data\n",
        "    \n",
        "    def parse_document(self, document_index):\n",
        "        document = metadata_with_docs.iloc[document_index]\n",
        "        \n",
        "        # One article can have multiple associated documents\n",
        "        words = []\n",
        "        for filename in document['pdf_json_files'].split('; '):\n",
        "            filepath = '{0}/{1}'.format(covid_dirpath, filename)\n",
        "            data = self.read_file_to_json(filepath)\n",
        "\n",
        "            # Split each paragraph into multiple sentences first, in order to improve the performance of the word tokenizer\n",
        "            text = data['body_text']\n",
        "            for paragraph in text:\n",
        "                p_sentences = sent_tokenize(paragraph['text'])\n",
        "\n",
        "                # Split each sentence into words, while making sure to remove the stopwords and stem the words\n",
        "                for p_sentence in p_sentences:\n",
        "                    sentence = [ self.stemmer.stem(word) for word in word_tokenize(p_sentence) if word.isalpha() and word.lower() not in stop_tokens ]\n",
        "                    words.extend(sentence)\n",
        "    \n",
        "        return (words, document['cord_uid'])\n",
        "        \n",
        "\n",
        "class Corpus:\n",
        "    \"\"\"An iterator that reads all sentences from the first N documents\"\"\"\n",
        "    \n",
        "    def __init__(self, n_documents):\n",
        "        self.n_documents = n_documents\n",
        "        self.stemmer = SnowballStemmer('english')\n",
        "        self.reader = Reader()\n",
        "        \n",
        "    def __iter__(self):\n",
        "         for document_index in range(0, self.n_documents):   \n",
        "            words, document_id = self.reader.parse_document(document_index)\n",
        "            yield TaggedDocument(words, document_id)\n",
        "            \n",
        "    def plain_iter(self):\n",
        "        for document_index in range(0, self.n_documents):  \n",
        "            words, document_id = self.reader.parse_document(document_index)\n",
        "            yield (words, document_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding documents as vectors\n",
        "\n",
        "In this lab, we're using a subset of 500 articles to train a Machine Learning model that encodes text documents into numerical vectors (a document embedding model). \n",
        "\n",
        "Training a document embedding model takes a significant amount of time, and for this reason we already provide a trained model. We also provide the code below in case you want to get more details about the process. Running the next two cells will result in loading the already existing model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "gather": {
          "logged": 1676793502816
        }
      },
      "outputs": [],
      "source": [
        "N_DOCUMENTS = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'covid_dirpath' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32m<timed exec>:8\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:296\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, shrink_windows, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_document_dm_concat\u001b[39m(model, doc_words, doctag_indexes, alpha, work\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, neu1\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, learn_doctags\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    255\u001b[0m                              learn_words\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, learn_hidden\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, word_vectors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, word_locks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    256\u001b[0m                              doctag_vectors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, doctag_locks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    257\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Update distributed memory model (\"PV-DM\") by training on a single document, using a\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39m    concatenation of the context window word vectors (rather than a sum or average). This\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[39m    might be slower since the input at each batch will be significantly larger.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \n\u001b[0;32m    261\u001b[0m \u001b[39m    Called internally from :meth:`~gensim.models.doc2vec.Doc2Vec.train` and\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39m    :meth:`~gensim.models.doc2vec.Doc2Vec.infer_vector`.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \n\u001b[0;32m    264\u001b[0m \u001b[39m    Notes\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m    -----\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[39m    This is the non-optimized, Python version. If you have cython installed, gensim\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[39m    will use the optimized version from :mod:`gensim.models.doc2vec_inner` instead.\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \n\u001b[0;32m    269\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39m    model : :class:`~gensim.models.doc2vec.Doc2Vec`\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[39m        The model to train.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[39m    doc_words : list of str\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[39m        The input document as a list of words to be used for training. Each word will be looked up in\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[39m        the model's vocabulary.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39m    doctag_indexes : list of int\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39m        Indices into `doctag_vectors` used to obtain the tags of the document.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[39m    alpha : float\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[39m        Learning rate.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[39m    work : object\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[39m        UNUSED.\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[39m    neu1 : object\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[39m        UNUSED.\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[39m    learn_doctags : bool, optional\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[39m        Whether the tag vectors should be updated.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39m    learn_words : bool, optional\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39m        Word vectors will be updated exactly as per Word2Vec skip-gram training only if **both**\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m        `learn_words` and `train_words` are set to True.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m    learn_hidden : bool, optional\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m        Whether or not the weights of the hidden layer will be updated.\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[39m    word_vectors : iterable of list of float, optional\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39m        Vector representations of each word in the model's vocabulary.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m    word_locks : listf of float, optional\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39m        Lock factors for each word in the vocabulary.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[39m    doctag_vectors : list of list of float, optional\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[39m        Vector representations of the tags. If None, these will be retrieved from the model.\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39m    doctag_locks : list of float, optional\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[39m        The lock factors for each tag.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \n\u001b[0;32m    300\u001b[0m \u001b[39m    Returns\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[39m    -------\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[39m    int\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[39m        Number of words in the input document that were actually used for training (they were found in the\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[39m        vocabulary and they were not discarded by negative sampling).\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    307\u001b[0m     \u001b[39mif\u001b[39;00m word_vectors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m         word_vectors \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39msyn0\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:429\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    425\u001b[0m neu1e \u001b[39m=\u001b[39m zeros(l1\u001b[39m.\u001b[39mshape)\n\u001b[0;32m    427\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mhs:\n\u001b[0;32m    428\u001b[0m     \u001b[39m# work on the entire tree at once, to push as much work into numpy's C routines as possible (performance)\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     l2a \u001b[39m=\u001b[39m deepcopy(model\u001b[39m.\u001b[39msyn1[predict_word\u001b[39m.\u001b[39mpoint])  \u001b[39m# 2d matrix, codelen x layer1_size\u001b[39;00m\n\u001b[0;32m    430\u001b[0m     prod_term \u001b[39m=\u001b[39m dot(l1, l2a\u001b[39m.\u001b[39mT)\n\u001b[0;32m    431\u001b[0m     fa \u001b[39m=\u001b[39m expit(prod_term)  \u001b[39m# propagate hidden -> output\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:882\u001b[0m, in \u001b[0;36mbuild_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mestimated_lookup_memory\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    876\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get estimated memory for tag lookup, 0 if using pure int tags.\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \n\u001b[0;32m    878\u001b[0m \u001b[39m    Returns\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[39m    -------\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[39m    int\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[39m        The estimated RAM required to look up a tag in bytes.\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m \n\u001b[0;32m    883\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    884\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m60\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocvecs\u001b[39m.\u001b[39moffset2doctag) \u001b[39m+\u001b[39m \u001b[39m140\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocvecs\u001b[39m.\u001b[39mdoctags)\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:1054\u001b[0m, in \u001b[0;36mscan_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_word2vec_format\u001b[39m(\u001b[39mself\u001b[39m, fname, doctag_vec\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, word_vec\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*dt_\u001b[39m\u001b[39m'\u001b[39m, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   1041\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Store the input-hidden weight matrix in the same format used by the original C word2vec-tool.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \n\u001b[0;32m   1043\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[0;32m   1045\u001b[0m \u001b[39m    fname : str\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \u001b[39m        The file path used to save the vectors in.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[39m    doctag_vec : bool, optional\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m        Indicates whether to store document vectors.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39m    word_vec : bool, optional\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[39m        Indicates whether to store word vectors.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[39m    prefix : str, optional\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m \u001b[39m        Uniquely identifies doctags from word vocab, and avoids collision in case of repeated string in doctag\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[39m        and word vocab.\u001b[39;00m\n\u001b[1;32m-> 1054\u001b[0m \u001b[39m    fvocab : str, optional\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[39m        Optional file path used to save the vocabulary.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[39m    binary : bool, optional\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m \u001b[39m        If True, the data will be saved in binary word2vec format, otherwise - will be saved in plain text.\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \n\u001b[0;32m   1059\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m     total_vec \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mvocab) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocvecs)\n\u001b[0;32m   1061\u001b[0m     write_first_line \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:954\u001b[0m, in \u001b[0;36m_scan_vocab\u001b[1;34m(self, corpus_iterable, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, tag):\n\u001b[0;32m    953\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the vector representation of (possible multi-term) tag.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m \n\u001b[0;32m    955\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m    956\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[39m    tag : {str, int, list of str, list of int}\u001b[39;00m\n\u001b[0;32m    958\u001b[0m \u001b[39m        The tag (or tags) to be looked up in the model.\u001b[39;00m\n\u001b[0;32m    959\u001b[0m \n\u001b[0;32m    960\u001b[0m \u001b[39m    Returns\u001b[39;00m\n\u001b[0;32m    961\u001b[0m \u001b[39m    -------\u001b[39;00m\n\u001b[0;32m    962\u001b[0m \u001b[39m    np.ndarray\u001b[39;00m\n\u001b[0;32m    963\u001b[0m \u001b[39m        The vector representations of each tag as a matrix (will be 1D if `tag` was a single tag)\u001b[39;00m\n\u001b[0;32m    964\u001b[0m \n\u001b[0;32m    965\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    966\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tag, string_types \u001b[39m+\u001b[39m integer_types \u001b[39m+\u001b[39m (integer,)):\n\u001b[0;32m    967\u001b[0m         \u001b[39mif\u001b[39;00m tag \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mvocab:\n",
            "Cell \u001b[1;32mIn[58], line 47\u001b[0m, in \u001b[0;36mCorpus.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     46\u001b[0m      \u001b[39mfor\u001b[39;00m document_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_documents):   \n\u001b[1;32m---> 47\u001b[0m         words, document_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreader\u001b[39m.\u001b[39;49mparse_document(document_index)\n\u001b[0;32m     48\u001b[0m         \u001b[39myield\u001b[39;00m TaggedDocument(words, document_id)\n",
            "Cell \u001b[1;32mIn[58], line 21\u001b[0m, in \u001b[0;36mReader.parse_document\u001b[1;34m(self, document_index)\u001b[0m\n\u001b[0;32m     19\u001b[0m words \u001b[39m=\u001b[39m []\n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m document[\u001b[39m'\u001b[39m\u001b[39mpdf_json_files\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m; \u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m     filepath \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(covid_dirpath, filename)\n\u001b[0;32m     22\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_file_to_json(filepath)\n\u001b[0;32m     24\u001b[0m     \u001b[39m# Split each paragraph into multiple sentences first, in order to improve the performance of the word tokenizer\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'covid_dirpath' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "model_filename = f'covid_embeddings_model_{N_DOCUMENTS}_docs.w2v'\n",
        "\n",
        "# if (os.path.exists(model_filename)):\n",
        "#     model = Doc2Vec.load(model_filename)\n",
        "#     print(f'Done, loaded word2vec model with { len(model.wv.vocab) } words.')\n",
        "# # else:\n",
        "model = Doc2Vec(Corpus(N_DOCUMENTS), vector_size=128, batch_words=10)\n",
        "model.save(model_filename)\n",
        "print(f'Done, trained word2vec model with { len(model.wv.vocab) } words.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word frequencies\n",
        "\n",
        "Let's analyze the relative frequencies of words in the corpus of articles. We will display a word cloud to provide a visual representation of these relative frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'covid_dirpath' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32m<timed exec>:4\u001b[0m\n",
            "Cell \u001b[1;32mIn[30], line 52\u001b[0m, in \u001b[0;36mCorpus.plain_iter\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplain_iter\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     51\u001b[0m     \u001b[39mfor\u001b[39;00m document_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_documents):  \n\u001b[1;32m---> 52\u001b[0m         words, document_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreader\u001b[39m.\u001b[39;49mparse_document(document_index)\n\u001b[0;32m     53\u001b[0m         \u001b[39myield\u001b[39;00m (words, document_id)\n",
            "Cell \u001b[1;32mIn[30], line 21\u001b[0m, in \u001b[0;36mReader.parse_document\u001b[1;34m(self, document_index)\u001b[0m\n\u001b[0;32m     19\u001b[0m words \u001b[39m=\u001b[39m []\n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m document[\u001b[39m'\u001b[39m\u001b[39mpdf_json_files\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m; \u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m     filepath \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(covid_dirpath, filename)\n\u001b[0;32m     22\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_file_to_json(filepath)\n\u001b[0;32m     24\u001b[0m     \u001b[39m# Split each paragraph into multiple sentences first, in order to improve the performance of the word tokenizer\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'covid_dirpath' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "word_vectors = []\n",
        "ids = []\n",
        "\n",
        "for (words, doc_id) in Corpus(N_DOCUMENTS).plain_iter():\n",
        "    ids.append(doc_id)\n",
        "    word_vector = model.infer_vector(words)\n",
        "    word_vectors.append(word_vector)\n",
        "    if len(word_vectors) % 100 == 0:\n",
        "        print(f'Processed {len(word_vectors)} documents.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we've finished reading the articles, we can dismount the dataset in order to free up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "gather": {
          "logged": 1676793905367
        }
      },
      "outputs": [],
      "source": [
        "#mount.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "gather": {
          "logged": 1676793905599
        }
      },
      "outputs": [],
      "source": [
        "wv_df = pd.DataFrame(word_vectors, index=ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll join the DataFrame containing the numerical embeddings with the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "gather": {
          "logged": 1676793905820
        }
      },
      "outputs": [],
      "source": [
        "indexed_metadata = metadata_with_docs.set_index('cord_uid')\n",
        "metadata_with_embeddings = pd.concat([indexed_metadata.iloc[:N_DOCUMENTS], wv_df], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering documents\n",
        "\n",
        "We've determined the acceptable value for the clusters, so let's use Machine Learning to determine those clusters. We'll use the classic KMeans algorithm to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "gather": {
          "logged": 1676793909538
        }
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "at least one array or dtype is required",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[65], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m visualizer \u001b[39m=\u001b[39m KElbowVisualizer(KMeans(), k\u001b[39m=\u001b[39m(\u001b[39m3\u001b[39m,\u001b[39m20\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m visualizer\u001b[39m.\u001b[39;49mfit(wv_df)\n\u001b[0;32m      3\u001b[0m \u001b[39m# clusterer = KMeans(12 if visualizer.elbow_value_ > 12 else visualizer.elbow_value_)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# clusterer.fit(wv_df)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# clusters = clusterer.labels_\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\yellowbrick\\cluster\\elbow.py:339\u001b[0m, in \u001b[0;36mKElbowVisualizer.fit\u001b[1;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39m# Set the k value and fit the model\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39mset_params(n_clusters\u001b[39m=\u001b[39mk)\n\u001b[1;32m--> 339\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    341\u001b[0m \u001b[39m# Append the time and score to our plottable metrics\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_timers_\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start)\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1417\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m \n\u001b[0;32m   1392\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[39m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m-> 1417\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m   1418\u001b[0m     X,\n\u001b[0;32m   1419\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1420\u001b[0m     dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[0;32m   1421\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1422\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy_x,\n\u001b[0;32m   1423\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1424\u001b[0m )\n\u001b[0;32m   1426\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m   1428\u001b[0m random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\sklearn\\base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 546\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    547\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    548\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:778\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    774\u001b[0m     pandas_requires_conversion \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(\n\u001b[0;32m    775\u001b[0m         _pandas_dtype_needs_early_conversion(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dtypes_orig\n\u001b[0;32m    776\u001b[0m     )\n\u001b[0;32m    777\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(dtype_iter, np\u001b[39m.\u001b[39mdtype) \u001b[39mfor\u001b[39;00m dtype_iter \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[1;32m--> 778\u001b[0m         dtype_orig \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mresult_type(\u001b[39m*\u001b[39;49mdtypes_orig)\n\u001b[0;32m    780\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(array, \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(array, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    781\u001b[0m     \u001b[39m# array is a pandas series\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     pandas_requires_conversion \u001b[39m=\u001b[39m _pandas_dtype_needs_early_conversion(array\u001b[39m.\u001b[39mdtype)\n",
            "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mresult_type\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: at least one array or dtype is required"
          ]
        }
      ],
      "source": [
        "visualizer = KElbowVisualizer(KMeans(), k=(3,20))\n",
        "visualizer.fit(wv_df)\n",
        "clusterer = KMeans(12 if visualizer.elbow_value_ > 12 else visualizer.elbow_value_)\n",
        "clusterer.fit(wv_df)\n",
        "clusters = clusterer.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll add each article's cluster as new column to our combined dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "gather": {
          "logged": 1676793909799
        }
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'clusters' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[49], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m metadata_with_clusters \u001b[39m=\u001b[39m metadata_with_embeddings\n\u001b[1;32m----> 2\u001b[0m metadata_with_clusters[\u001b[39m'\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m clusters\n\u001b[0;32m      3\u001b[0m metadata_with_clusters\n",
            "\u001b[1;31mNameError\u001b[0m: name 'clusters' is not defined"
          ]
        }
      ],
      "source": [
        "metadata_with_clusters = metadata_with_embeddings\n",
        "metadata_with_clusters['cluster'] = clusters\n",
        "metadata_with_clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now split our data into two datasets - a **training** one that will be used to train a Machine Learning model, able to determine the cluster that should be assigned to an article, and a **test** one that we'll use to test this classifier.\n",
        "\n",
        "We will allocate 80% of the articles to training the Machine Learning model, and the remaining 20% to testing it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676793910073
        }
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(metadata_with_clusters, train_size=0.8)\n",
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To speed up training, we'll ignore all columns except the word vectors calculated using Doc2Vec. For this reason, we will create a separate dataset just with the vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676793910280
        }
      },
      "outputs": [],
      "source": [
        "columns_to_ignore = ['sha', 'source_x', 'title', 'doi', 'pmcid', 'pubmed_id', 'license', 'abstract', 'publish_time', 'authors', 'journal', 'mag_id',\n",
        "                     'who_covidence_id', 'arxiv_id', 'pdf_json_files', 'pmc_json_files', 'url', 's2_id' ]\n",
        "train_data_vectors = train.drop(columns_to_ignore, axis=1)\n",
        "test_data_vectors = test.drop(columns_to_ignore, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register the training and testing datasets for AutoML availability\n",
        "\n",
        "We're registering the training and testing datasets with the Azure Machine Learning datastore to make them available inside Azure Machine Learning Studio and Automated ML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676793914243
        }
      },
      "outputs": [],
      "source": [
        "# Retrieve your ML workspace\n",
        "ws = Workspace.from_config()\n",
        "# Retrieve the workspace's default datastore\n",
        "datastore = ws.get_default_datastore()\n",
        "\n",
        "Dataset.Tabular.register_pandas_dataframe(train_data_vectors, datastore, 'COVID19Articles_Train')\n",
        "Dataset.Tabular.register_pandas_dataframe(test_data_vectors, datastore, 'COVID19Articles_Test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676794165844
        }
      },
      "outputs": [],
      "source": [
        "# The name of the compute instance\n",
        "compute_name = 'aml-compute-cpu'\n",
        "# The minimum and maximum number of nodes of the compute instance\n",
        "compute_min_nodes = 1\n",
        "# Setting the number of maximum nodes to a higher value will allow Automated ML to run more experiments in parallel, but will also inccrease your costs\n",
        "compute_max_nodes = 1\n",
        "\n",
        "vm_size = 'STANDARD_DS3_V2'\n",
        "\n",
        "# Check existing compute targets in the workspace for a compute with this name\n",
        "if compute_name in ws.compute_targets:\n",
        "    compute_target = ws.compute_targets[compute_name]\n",
        "    if compute_target and type(compute_target) is AmlCompute:\n",
        "        print(f'Found existing compute target: {compute_name}')    \n",
        "else:\n",
        "    print(f'A new compute target is needed: {compute_name}')\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
        "                                                                min_nodes = compute_min_nodes, \n",
        "                                                                max_nodes = compute_max_nodes)\n",
        "\n",
        "    # Create the cluster\n",
        "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
        "    \n",
        "    # Wait for provisioning to complete\n",
        "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676794166087
        }
      },
      "outputs": [],
      "source": [
        "# Retrieve the COVID19Articles_Train dataset from the workspace\n",
        "train_data = Dataset.get_by_name(ws, 'COVID19Articles_Train')\n",
        "\n",
        "\n",
        "\n",
        "# Configura Automated ML\n",
        "automl_config = AutoMLConfig(task = \"classification\",\n",
        "                             # Use weighted area under curve metric to evaluate the models\n",
        "                             primary_metric='AUC_weighted',\n",
        "                             \n",
        "                             # Use all columns except the ones we decided to ignore\n",
        "                             training_data = train_data,\n",
        "                             \n",
        "                             # The values we're trying to predict are in the `cluster` column\n",
        "                             label_column_name = 'cluster',\n",
        "                             \n",
        "                             # Evaluate the model with 5-fold cross validation\n",
        "                             n_cross_validations=5,\n",
        "                             \n",
        "                             # The experiment should be stopped after 15 minutes, to minimize cost\n",
        "                             experiment_timeout_hours=.25,\n",
        "                             #blocked_models=['XGBoostClassifier'],\n",
        "                             \n",
        "                             # Automated ML can try at most 1 models at the same time, this is also limited by the compute instance's maximum number of nodes\n",
        "                             max_concurrent_iterations=1,\n",
        "                             \n",
        "                             # An iteration should be stopped if it takes more than 5 minutes\n",
        "                             iteration_timeout_minutes=3,\n",
        "                             \n",
        "                             compute_target=compute_target,\n",
        "                             \n",
        "                             #The total number of different algorithm and parameter combinations to test during an automated ML experiment. If not specified, the default is 1000 iterations.\n",
        "                             iterations = 5\n",
        "                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676795096774
        }
      },
      "outputs": [],
      "source": [
        "# Use the `COVID19Articles_Train_Vectors` dataset\n",
        "exp = Experiment(ws, 'COVID19_Classification')\n",
        "run = exp.submit(automl_config, show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676795414027
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Retrieve the best performing run and its corresponding model from the aggregated Automated ML run\n",
        "best_run, best_model = run.get_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676795355958
        }
      },
      "outputs": [],
      "source": [
        "RunDetails(run).show()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "213055c14b8909d2b6639a9de47aeb416d9686ed66cda418a2d2d092e84a2dc1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
