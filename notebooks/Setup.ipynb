{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to the COVID-19 Open Research Dataset\n",
        "\n",
        "The COVID-19 Open Research Dataset (CORD-19) is a collection of over 50,000 scholarly articles - including over 40,000 with full text - about COVID-19, SARS-CoV-2, and related coronaviruses. This dataset has been made freely available with the goal to aid research communities combat the COVID-19 pandemic. It has been made available by the Allen Institute for AI in partnership with leading research groups to prepare and distribute the COVID-19 Open Research Dataset (CORD-19), in response to the COVID-19 pandemic.\n",
        "\n",
        "During this lab you will learn how to process and analyze a subset of the articles present in the dataset, group them together into a series of clusters, and use Automated ML to train a machine learning model capable of classifying new articles as they are published."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n",
        "\n",
        "We will start off by installing a few packages, such as `nltk` for text processing and `wordcloud`, `seaborn`, and `yellowbrick` for various visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim==3.8.2 in c:\\users\\photo\\appdata\\roaming\\python\\python310\\site-packages (3.8.2)\n",
            "Requirement already satisfied: six>=1.5.0 in c:\\users\\photo\\miniconda3\\lib\\site-packages (from gensim==3.8.2) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\photo\\appdata\\roaming\\python\\python310\\site-packages (from gensim==3.8.2) (1.23.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\photo\\miniconda3\\lib\\site-packages (from gensim==3.8.2) (6.3.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\photo\\miniconda3\\lib\\site-packages (from gensim==3.8.2) (1.10.1)\n"
          ]
        }
      ],
      "source": [
        "# !pip install nltk\n",
        "# !pip install wordcloud\n",
        "# !pip install seaborn\n",
        "# !pip install yellowbrick\n",
        "!pip install  gensim==3.8.2\n",
        "# !pip install azureml-core \n",
        "# !pip install azureml-widgets "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll first download stopwords and the Punkt tokenizer models present in the `nltk` package, in order to be able to process the articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "gather": {
          "logged": 1676793367486
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\photo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\photo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll also import the rest of the modules needed in this notebook, and do a quick sanity-check on the Azure ML SDK version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "gather": {
          "logged": 1676793371262
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Azure ML SDK Version:  1.49.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from string import punctuation\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set_palette('Set2')\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans, SpectralClustering, DBSCAN, Birch, AgglomerativeClustering\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.stem import SnowballStemmer, PorterStemmer\n",
        "\n",
        "from azureml.core import Workspace, Datastore, Dataset, VERSION\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core import Dataset, Workspace, Experiment\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.train.automl.run import AutoMLRun\n",
        "from azureml.widgets import RunDetails\n",
        "from azureml.automl.core.featurization.featurizationconfig import FeaturizationConfig\n",
        "\n",
        "print(\"Azure ML SDK Version: \", VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the Covid-19 data\n",
        "\n",
        "CORD-19 has been uploaded as an Azure Open Dataset, we will connect to it and use it's API to download the dataset locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "gather": {
          "logged": 1676793381332
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You already Downloaded teh metadata.csv file. We will use that one.\n"
          ]
        }
      ],
      "source": [
        "baseUrl = \"https://stgai2023.blob.core.windows.net/opendata/\"\n",
        "import requests\n",
        "if not os.path.exists(\"metadata.csv\"):\n",
        "    print(\"Hold on as I pull 1.5Gigs of data...It'll be worth it!\")\n",
        "    response = requests.request(\"GET\", baseUrl+\"metadata.csv\")\n",
        "    csvData = response.text\n",
        "    file = open('./metadata.csv', 'w')\n",
        "    file.write(csvData)\n",
        "    file.close()\n",
        "    print(\"Got it. Woah. That's some data!\")\n",
        "else:\n",
        "    print(\"You already Downloaded teh metadata.csv file. We will use that one.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display a sample of the dataset (top 5 rows)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "gather": {
          "logged": 1676793413126
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\photo\\AppData\\Local\\Temp\\ipykernel_20428\\1400788635.py:1: DtypeWarning: Columns (1,4,5,6,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  metadata = pd.read_csv(\"./metadata.csv\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cord_uid</th>\n",
              "      <th>sha</th>\n",
              "      <th>source_x</th>\n",
              "      <th>title</th>\n",
              "      <th>doi</th>\n",
              "      <th>pmcid</th>\n",
              "      <th>pubmed_id</th>\n",
              "      <th>license</th>\n",
              "      <th>abstract</th>\n",
              "      <th>publish_time</th>\n",
              "      <th>authors</th>\n",
              "      <th>journal</th>\n",
              "      <th>mag_id</th>\n",
              "      <th>who_covidence_id</th>\n",
              "      <th>arxiv_id</th>\n",
              "      <th>pdf_json_files</th>\n",
              "      <th>pmc_json_files</th>\n",
              "      <th>url</th>\n",
              "      <th>s2_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ug7v899j</td>\n",
              "      <td>d1aafb70c066a2068b02786f8929fd9c900897fb</td>\n",
              "      <td>PMC</td>\n",
              "      <td>Clinical features of culture-proven Mycoplasma...</td>\n",
              "      <td>10.1186/1471-2334-1-6</td>\n",
              "      <td>PMC35282</td>\n",
              "      <td>11472636</td>\n",
              "      <td>no-cc</td>\n",
              "      <td>OBJECTIVE: This retrospective chart review des...</td>\n",
              "      <td>2001-07-04</td>\n",
              "      <td>Madani, Tariq A; Al-Ghamdi, Aisha A</td>\n",
              "      <td>BMC Infect Dis</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>document_parses/pdf_json/d1aafb70c066a2068b027...</td>\n",
              "      <td>document_parses/pmc_json/PMC35282.xml.json</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>02tnwd4m</td>\n",
              "      <td>6b0567729c2143a66d737eb0a2f63f2dce2e5a7d</td>\n",
              "      <td>PMC</td>\n",
              "      <td>Nitric oxide: a pro-inflammatory mediator in l...</td>\n",
              "      <td>10.1186/rr14</td>\n",
              "      <td>PMC59543</td>\n",
              "      <td>11667967</td>\n",
              "      <td>no-cc</td>\n",
              "      <td>Inflammatory diseases of the respiratory tract...</td>\n",
              "      <td>2000-08-15</td>\n",
              "      <td>Vliet, Albert van der; Eiserich, Jason P; Cros...</td>\n",
              "      <td>Respir Res</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>document_parses/pdf_json/6b0567729c2143a66d737...</td>\n",
              "      <td>document_parses/pmc_json/PMC59543.xml.json</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ejv2xln0</td>\n",
              "      <td>06ced00a5fc04215949aa72528f2eeaae1d58927</td>\n",
              "      <td>PMC</td>\n",
              "      <td>Surfactant protein-D and pulmonary host defense</td>\n",
              "      <td>10.1186/rr19</td>\n",
              "      <td>PMC59549</td>\n",
              "      <td>11667972</td>\n",
              "      <td>no-cc</td>\n",
              "      <td>Surfactant protein-D (SP-D) participates in th...</td>\n",
              "      <td>2000-08-25</td>\n",
              "      <td>Crouch, Erika C</td>\n",
              "      <td>Respir Res</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>document_parses/pdf_json/06ced00a5fc04215949aa...</td>\n",
              "      <td>document_parses/pmc_json/PMC59549.xml.json</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2b73a28n</td>\n",
              "      <td>348055649b6b8cf2b9a376498df9bf41f7123605</td>\n",
              "      <td>PMC</td>\n",
              "      <td>Role of endothelin-1 in lung disease</td>\n",
              "      <td>10.1186/rr44</td>\n",
              "      <td>PMC59574</td>\n",
              "      <td>11686871</td>\n",
              "      <td>no-cc</td>\n",
              "      <td>Endothelin-1 (ET-1) is a 21 amino acid peptide...</td>\n",
              "      <td>2001-02-22</td>\n",
              "      <td>Fagan, Karen A; McMurtry, Ivan F; Rodman, David M</td>\n",
              "      <td>Respir Res</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>document_parses/pdf_json/348055649b6b8cf2b9a37...</td>\n",
              "      <td>document_parses/pmc_json/PMC59574.xml.json</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9785vg6d</td>\n",
              "      <td>5f48792a5fa08bed9f56016f4981ae2ca6031b32</td>\n",
              "      <td>PMC</td>\n",
              "      <td>Gene expression in epithelial cells in respons...</td>\n",
              "      <td>10.1186/rr61</td>\n",
              "      <td>PMC59580</td>\n",
              "      <td>11686888</td>\n",
              "      <td>no-cc</td>\n",
              "      <td>Respiratory syncytial virus (RSV) and pneumoni...</td>\n",
              "      <td>2001-05-11</td>\n",
              "      <td>Domachowske, Joseph B; Bonville, Cynthia A; Ro...</td>\n",
              "      <td>Respir Res</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>document_parses/pdf_json/5f48792a5fa08bed9f560...</td>\n",
              "      <td>document_parses/pmc_json/PMC59580.xml.json</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cord_uid                                       sha source_x  \\\n",
              "0  ug7v899j  d1aafb70c066a2068b02786f8929fd9c900897fb      PMC   \n",
              "1  02tnwd4m  6b0567729c2143a66d737eb0a2f63f2dce2e5a7d      PMC   \n",
              "2  ejv2xln0  06ced00a5fc04215949aa72528f2eeaae1d58927      PMC   \n",
              "3  2b73a28n  348055649b6b8cf2b9a376498df9bf41f7123605      PMC   \n",
              "4  9785vg6d  5f48792a5fa08bed9f56016f4981ae2ca6031b32      PMC   \n",
              "\n",
              "                                               title                    doi  \\\n",
              "0  Clinical features of culture-proven Mycoplasma...  10.1186/1471-2334-1-6   \n",
              "1  Nitric oxide: a pro-inflammatory mediator in l...           10.1186/rr14   \n",
              "2    Surfactant protein-D and pulmonary host defense           10.1186/rr19   \n",
              "3               Role of endothelin-1 in lung disease           10.1186/rr44   \n",
              "4  Gene expression in epithelial cells in respons...           10.1186/rr61   \n",
              "\n",
              "      pmcid pubmed_id license  \\\n",
              "0  PMC35282  11472636   no-cc   \n",
              "1  PMC59543  11667967   no-cc   \n",
              "2  PMC59549  11667972   no-cc   \n",
              "3  PMC59574  11686871   no-cc   \n",
              "4  PMC59580  11686888   no-cc   \n",
              "\n",
              "                                            abstract publish_time  \\\n",
              "0  OBJECTIVE: This retrospective chart review des...   2001-07-04   \n",
              "1  Inflammatory diseases of the respiratory tract...   2000-08-15   \n",
              "2  Surfactant protein-D (SP-D) participates in th...   2000-08-25   \n",
              "3  Endothelin-1 (ET-1) is a 21 amino acid peptide...   2001-02-22   \n",
              "4  Respiratory syncytial virus (RSV) and pneumoni...   2001-05-11   \n",
              "\n",
              "                                             authors         journal  mag_id  \\\n",
              "0                Madani, Tariq A; Al-Ghamdi, Aisha A  BMC Infect Dis     NaN   \n",
              "1  Vliet, Albert van der; Eiserich, Jason P; Cros...      Respir Res     NaN   \n",
              "2                                    Crouch, Erika C      Respir Res     NaN   \n",
              "3  Fagan, Karen A; McMurtry, Ivan F; Rodman, David M      Respir Res     NaN   \n",
              "4  Domachowske, Joseph B; Bonville, Cynthia A; Ro...      Respir Res     NaN   \n",
              "\n",
              "  who_covidence_id arxiv_id  \\\n",
              "0              NaN      NaN   \n",
              "1              NaN      NaN   \n",
              "2              NaN      NaN   \n",
              "3              NaN      NaN   \n",
              "4              NaN      NaN   \n",
              "\n",
              "                                      pdf_json_files  \\\n",
              "0  document_parses/pdf_json/d1aafb70c066a2068b027...   \n",
              "1  document_parses/pdf_json/6b0567729c2143a66d737...   \n",
              "2  document_parses/pdf_json/06ced00a5fc04215949aa...   \n",
              "3  document_parses/pdf_json/348055649b6b8cf2b9a37...   \n",
              "4  document_parses/pdf_json/5f48792a5fa08bed9f560...   \n",
              "\n",
              "                               pmc_json_files  \\\n",
              "0  document_parses/pmc_json/PMC35282.xml.json   \n",
              "1  document_parses/pmc_json/PMC59543.xml.json   \n",
              "2  document_parses/pmc_json/PMC59549.xml.json   \n",
              "3  document_parses/pmc_json/PMC59574.xml.json   \n",
              "4  document_parses/pmc_json/PMC59580.xml.json   \n",
              "\n",
              "                                                 url  s2_id  \n",
              "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...    NaN  \n",
              "1  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
              "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
              "3  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
              "4  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  "
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metadata = pd.read_csv(\"./metadata.csv\")\n",
        "metadata.head(5) # let's see the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some of the articles do not have any associated documents, so we will filter those out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "gather": {
          "logged": 1676793413600
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset contains 1056660 entries, out of which 373766 have associated json documents\n"
          ]
        }
      ],
      "source": [
        "metadata_with_docs = metadata[metadata['pdf_json_files'].isna() == False]\n",
        "\n",
        "print(f'Dataset contains {metadata.shape[0]} entries, out of which {metadata_with_docs.shape[0]} have associated json documents')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the percentage of items in the dataset that have associated JSON documents (research papers that have extra metadata associated with them)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "gather": {
          "logged": 1676793413942
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document local filepath: https://stgai2023.blob.core.windows.net/opendata/document_parses/pdf_json/d1aafb70c066a2068b02786f8929fd9c900897fb.json\n"
          ]
        }
      ],
      "source": [
        "# Change the document index in order to preview a different article\n",
        "DOCUMENT_INDEX = 0 \n",
        "example_entry = metadata_with_docs.iloc[DOCUMENT_INDEX]\n",
        "jsonDatauri = baseUrl+example_entry[\"pdf_json_files\"]\n",
        "\n",
        "# filepath = os.path.join(covid_dirpath, example_entry['pdf_json_files'])\n",
        "print(f'Document local filepath: {filepath}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will display the list of elements that are available for the selected document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1676793415974
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data elements: paper_id, metadata, abstract, body_text, bib_entries, ref_entries, back_matter\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "response = requests.request(\"GET\", jsonDatauri)\n",
        "data = response.json()\n",
        "        \n",
        "print(f'Data elements: { \", \".join(data.keys())}' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "View the full text version of the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "gather": {
          "logged": 1676793495625
        }
      },
      "outputs": [],
      "source": [
        "stop_tokens = nltk.corpus.stopwords.words('english') + list(punctuation) + ['et', 'al.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "gather": {
          "logged": 1676793499553
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "covid_dirpath = \"./metadata.csv\"\n",
        "\n",
        "class Reader:\n",
        "    \"\"\"Class used to read the files associated with an article\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.stemmer = SnowballStemmer('english')\n",
        "    \n",
        "    def read_file_to_json(self, filepath):\n",
        "        response = requests.request(\"GET\", jsonDatauri)\n",
        "        data = response.json()        \n",
        "        return data\n",
        "    \n",
        "    def parse_document(self, document_index):\n",
        "        document = metadata_with_docs.iloc[document_index]\n",
        "        \n",
        "        # One article can have multiple associated documents\n",
        "        words = []\n",
        "        for filename in document['pdf_json_files'].split('; '):\n",
        "            filepath = '{0}/{1}'.format(covid_dirpath, filename)\n",
        "            data = self.read_file_to_json(filepath)\n",
        "\n",
        "            # Split each paragraph into multiple sentences first, in order to improve the performance of the word tokenizer\n",
        "            text = data['body_text']\n",
        "            for paragraph in text:\n",
        "                p_sentences = sent_tokenize(paragraph['text'])\n",
        "\n",
        "                # Split each sentence into words, while making sure to remove the stopwords and stem the words\n",
        "                for p_sentence in p_sentences:\n",
        "                    sentence = [ self.stemmer.stem(word) for word in word_tokenize(p_sentence) if word.isalpha() and word.lower() not in stop_tokens ]\n",
        "                    words.extend(sentence)\n",
        "    \n",
        "        return (words, document['cord_uid'])\n",
        "        \n",
        "\n",
        "class Corpus:\n",
        "    \"\"\"An iterator that reads all sentences from the first N documents\"\"\"\n",
        "    \n",
        "    def __init__(self, n_documents):\n",
        "        self.n_documents = n_documents\n",
        "        self.stemmer = SnowballStemmer('english')\n",
        "        self.reader = Reader()\n",
        "        \n",
        "    def __iter__(self):\n",
        "         for document_index in range(0, self.n_documents):   \n",
        "            words, document_id = self.reader.parse_document(document_index)\n",
        "            yield TaggedDocument(words, document_id)\n",
        "            \n",
        "    def plain_iter(self):\n",
        "        for document_index in range(0, self.n_documents):  \n",
        "            words, document_id = self.reader.parse_document(document_index)\n",
        "            yield (words, document_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding documents as vectors\n",
        "\n",
        "In this lab, we're using a subset of 500 articles to train a Machine Learning model that encodes text documents into numerical vectors (a document embedding model). \n",
        "\n",
        "Training a document embedding model takes a significant amount of time, and for this reason we already provide a trained model. We also provide the code below in case you want to get more details about the process. Running the next two cells will result in loading the already existing model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "gather": {
          "logged": 1676793502816
        }
      },
      "outputs": [],
      "source": [
        "N_DOCUMENTS = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[1;32m<timed exec>:8\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:296\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, shrink_windows, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_document_dm_concat\u001b[39m(model, doc_words, doctag_indexes, alpha, work\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, neu1\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, learn_doctags\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    255\u001b[0m                              learn_words\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, learn_hidden\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, word_vectors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, word_locks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    256\u001b[0m                              doctag_vectors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, doctag_locks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    257\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Update distributed memory model (\"PV-DM\") by training on a single document, using a\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39m    concatenation of the context window word vectors (rather than a sum or average). This\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[39m    might be slower since the input at each batch will be significantly larger.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \n\u001b[0;32m    261\u001b[0m \u001b[39m    Called internally from :meth:`~gensim.models.doc2vec.Doc2Vec.train` and\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39m    :meth:`~gensim.models.doc2vec.Doc2Vec.infer_vector`.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \n\u001b[0;32m    264\u001b[0m \u001b[39m    Notes\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m    -----\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[39m    This is the non-optimized, Python version. If you have cython installed, gensim\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[39m    will use the optimized version from :mod:`gensim.models.doc2vec_inner` instead.\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \n\u001b[0;32m    269\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39m    model : :class:`~gensim.models.doc2vec.Doc2Vec`\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[39m        The model to train.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[39m    doc_words : list of str\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[39m        The input document as a list of words to be used for training. Each word will be looked up in\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[39m        the model's vocabulary.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39m    doctag_indexes : list of int\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39m        Indices into `doctag_vectors` used to obtain the tags of the document.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[39m    alpha : float\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[39m        Learning rate.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[39m    work : object\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[39m        UNUSED.\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[39m    neu1 : object\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[39m        UNUSED.\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[39m    learn_doctags : bool, optional\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[39m        Whether the tag vectors should be updated.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39m    learn_words : bool, optional\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39m        Word vectors will be updated exactly as per Word2Vec skip-gram training only if **both**\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m        `learn_words` and `train_words` are set to True.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m    learn_hidden : bool, optional\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m        Whether or not the weights of the hidden layer will be updated.\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[39m    word_vectors : iterable of list of float, optional\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39m        Vector representations of each word in the model's vocabulary.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m    word_locks : listf of float, optional\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39m        Lock factors for each word in the vocabulary.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[39m    doctag_vectors : list of list of float, optional\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[39m        Vector representations of the tags. If None, these will be retrieved from the model.\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39m    doctag_locks : list of float, optional\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[39m        The lock factors for each tag.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \n\u001b[0;32m    300\u001b[0m \u001b[39m    Returns\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[39m    -------\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[39m    int\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[39m        Number of words in the input document that were actually used for training (they were found in the\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[39m        vocabulary and they were not discarded by negative sampling).\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    307\u001b[0m     \u001b[39mif\u001b[39;00m word_vectors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m         word_vectors \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39msyn0\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:430\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mhs:\n\u001b[0;32m    428\u001b[0m     \u001b[39m# work on the entire tree at once, to push as much work into numpy's C routines as possible (performance)\u001b[39;00m\n\u001b[0;32m    429\u001b[0m     l2a \u001b[39m=\u001b[39m deepcopy(model\u001b[39m.\u001b[39msyn1[predict_word\u001b[39m.\u001b[39mpoint])  \u001b[39m# 2d matrix, codelen x layer1_size\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m     prod_term \u001b[39m=\u001b[39m dot(l1, l2a\u001b[39m.\u001b[39mT)\n\u001b[0;32m    431\u001b[0m     fa \u001b[39m=\u001b[39m expit(prod_term)  \u001b[39m# propagate hidden -> output\u001b[39;00m\n\u001b[0;32m    432\u001b[0m     ga \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m predict_word\u001b[39m.\u001b[39mcode \u001b[39m-\u001b[39m fa) \u001b[39m*\u001b[39m alpha  \u001b[39m# vector of error gradients multiplied by the learning rate\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:516\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, documents\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, corpus_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dm_mean\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dm\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, dbow_words\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, dm_concat\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m    471\u001b[0m              dm_tag_count\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, docvecs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, docvecs_mapfile\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, comment\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, trim_rule\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, callbacks\u001b[39m=\u001b[39m(),\n\u001b[0;32m    472\u001b[0m              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    473\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \n\u001b[0;32m    475\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39m    documents : iterable of list of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \u001b[39m        Input corpus, can be simply a list of elements, but for larger corpora,consider an iterable that streams\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[39m        the documents directly from disk/network. If you don't supply `documents` (or `corpus_file`), the model is\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[39m        left uninitialized -- use if you plan to initialize it in some other way.\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[39m    corpus_file : str, optional\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[39m        Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[39m        You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[39m        `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[39m        Documents' tags are assigned automatically and are equal to line number, as in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[39m        :class:`~gensim.models.doc2vec.TaggedLineDocument`.\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[39m    dm : {1,0}, optional\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[39m        Defines the training algorithm. If `dm=1`, 'distributed memory' (PV-DM) is used.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[39m        Otherwise, `distributed bag of words` (PV-DBOW) is employed.\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[39m    vector_size : int, optional\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[39m        Dimensionality of the feature vectors.\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[39m    window : int, optional\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[39m        The maximum distance between the current and predicted word within a sentence.\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39m    alpha : float, optional\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \u001b[39m        The initial learning rate.\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[39m    min_alpha : float, optional\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39m        Learning rate will linearly drop to `min_alpha` as training progresses.\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[39m    seed : int, optional\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[39m        Seed for the random number generator. Initial vectors for each word are seeded with a hash of\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[39m        the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[39m        you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[39m        from OS thread scheduling.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[39m        In Python 3, reproducibility between interpreter launches also requires use of the `PYTHONHASHSEED`\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[39m        environment variable to control hash randomization.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m    min_count : int, optional\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[39m        Ignores all words with total frequency lower than this.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[39m    max_vocab_size : int, optional\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[39m        Limits the RAM during vocabulary building; if there are more unique\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39m        words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[39m        Set to `None` for no limit.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[39m    sample : float, optional\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[39m        The threshold for configuring which higher-frequency words are randomly downsampled,\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[39m        useful range is (0, 1e-5).\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[39m    workers : int, optional\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[39m        Use these many worker threads to train the model (=faster training with multicore machines).\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m \u001b[39m    epochs : int, optional\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[39m        Number of iterations (epochs) over the corpus.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[39m    hs : {1,0}, optional\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[39m        If 1, hierarchical softmax will be used for model training.\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[39m        If set to 0, and `negative` is non-zero, negative sampling will be used.\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[39m    negative : int, optional\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[39m        If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[39m        should be drawn (usually between 5-20).\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[39m        If set to 0, no negative sampling is used.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[39m    ns_exponent : float, optional\u001b[39;00m\n\u001b[0;32m    526\u001b[0m \u001b[39m        The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[39m        to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[39m        than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[39m        More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[39m        other values may perform better for recommendation applications.\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \u001b[39m    dm_mean : {1,0}, optional\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \u001b[39m        If 0 , use the sum of the context word vectors. If 1, use the mean.\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[39m        Only applies when `dm` is used in non-concatenative mode.\u001b[39;00m\n\u001b[0;32m    534\u001b[0m \u001b[39m    dm_concat : {1,0}, optional\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[39m        If 1, use concatenation of context vectors rather than sum/average;\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[39m        Note concatenation results in a much-larger model, as the input\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[39m        is no longer the size of one (sampled or arithmetically combined) word vector, but the\u001b[39;00m\n\u001b[0;32m    538\u001b[0m \u001b[39m        size of the tag(s) and all words in the context strung together.\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[39m    dm_tag_count : int, optional\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[39m        Expected constant number of document tags per document, when using\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39m        dm_concat mode.\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[39m    dbow_words : {1,0}, optional\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[39m        If set to 1 trains word-vectors (in skip-gram fashion) simultaneous with DBOW\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[39m        doc-vector training; If 0, only trains doc-vectors (faster).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[39m    trim_rule : function, optional\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[39m        Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[39m        be trimmed away, or handled using the default (discard if word count < min_count).\u001b[39;00m\n\u001b[0;32m    548\u001b[0m \u001b[39m        Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[39m        or a callable that accepts parameters (word, count, min_count) and returns either\u001b[39;00m\n\u001b[0;32m    550\u001b[0m \u001b[39m        :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[39m        The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[39m        of the model.\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \n\u001b[0;32m    554\u001b[0m \u001b[39m        The input parameters are of the following types:\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[39m            * `word` (str) - the word we are examining\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[39m            * `count` (int) - the word's frequency count in the corpus\u001b[39;00m\n\u001b[0;32m    557\u001b[0m \u001b[39m            * `min_count` (int) - the minimum count threshold.\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \n\u001b[0;32m    559\u001b[0m \u001b[39m    callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[39m        List of callbacks that need to be executed/run at specific stages during training.\u001b[39;00m\n\u001b[0;32m    561\u001b[0m \n\u001b[0;32m    562\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    563\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39msentences\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    564\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mDeprecationWarning\u001b[39;00m(\n\u001b[0;32m    565\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39msentences\u001b[39m\u001b[39m'\u001b[39m\u001b[39m was renamed to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdocuments\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, and will be removed in 4.0.0, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39muse \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdocuments\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:1073\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mintersect_word2vec_format\u001b[39m(\u001b[39mself\u001b[39m, fname, lockf\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m   1056\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m \u001b[39m    where it intersects with the current vocabulary.\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \n\u001b[0;32m   1059\u001b[0m \u001b[39m    No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[39m    non-intersecting words are left alone.\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \n\u001b[0;32m   1062\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[39m    fname : str\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \u001b[39m        The file path to load the vectors from.\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[39m    lockf : float, optional\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[39m        Lock-factor value to be set for any imported word-vectors; the\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \u001b[39m        default value of 0.0 prevents further updating of the vector during subsequent\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[39m        training. Use 1.0 to allow further training updates of merged vectors.\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[39m    binary : bool, optional\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[39m        If True, `fname` is in the binary word2vec C format.\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[39m    encoding : str, optional\u001b[39;00m\n\u001b[1;32m-> 1073\u001b[0m \u001b[39m        Encoding of `text` for `unicode` function (python2 only).\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m \u001b[39m    unicode_errors : str, optional\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m \u001b[39m        Error handling behaviour, used as parameter for `unicode` function (python2 only).\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \n\u001b[0;32m   1077\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     overlap_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   1079\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading projection weights from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, fname)\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:1434\u001b[0m, in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1432\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n\u001b[0;32m   1433\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mislice(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlimit):\n\u001b[1;32m-> 1434\u001b[0m     line \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mto_unicode(line)\u001b[39m.\u001b[39msplit()\n\u001b[0;32m   1435\u001b[0m     i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   1436\u001b[0m     \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(line):\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:1289\u001b[0m, in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainables, \u001b[39m'\u001b[39m\u001b[39msyn1\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m save_syn1:\n\u001b[0;32m   1288\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainables\u001b[39m.\u001b[39msyn1\n\u001b[1;32m-> 1289\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainables, \u001b[39m'\u001b[39m\u001b[39msyn1neg\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m save_syn1neg:\n\u001b[0;32m   1290\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainables\u001b[39m.\u001b[39msyn1neg\n\u001b[0;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainables, \u001b[39m'\u001b[39m\u001b[39mvectors_lockf\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m save_vectors_lockf:\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[1;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[0;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "model_filename = f'covid_embeddings_model_{N_DOCUMENTS}_docs.w2v'\n",
        "\n",
        "if (os.path.exists(model_filename)):\n",
        "    model = Doc2Vec.load(model_filename)\n",
        "    print(f'Done, loaded word2vec model with { len(model.wv.vocab) } words.')\n",
        "else:\n",
        "    model = Doc2Vec(Corpus(N_DOCUMENTS), vector_size=128, batch_words=10)\n",
        "    model.save(model_filename)\n",
        "    print(f'Done, trained word2vec model with { len(model.wv.vocab) } words.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word frequencies\n",
        "\n",
        "Let's analyze the relative frequencies of words in the corpus of articles. We will display a word cloud to provide a visual representation of these relative frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'covid_dirpath' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32m<timed exec>:4\u001b[0m\n",
            "Cell \u001b[1;32mIn[30], line 52\u001b[0m, in \u001b[0;36mCorpus.plain_iter\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplain_iter\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     51\u001b[0m     \u001b[39mfor\u001b[39;00m document_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_documents):  \n\u001b[1;32m---> 52\u001b[0m         words, document_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreader\u001b[39m.\u001b[39;49mparse_document(document_index)\n\u001b[0;32m     53\u001b[0m         \u001b[39myield\u001b[39;00m (words, document_id)\n",
            "Cell \u001b[1;32mIn[30], line 21\u001b[0m, in \u001b[0;36mReader.parse_document\u001b[1;34m(self, document_index)\u001b[0m\n\u001b[0;32m     19\u001b[0m words \u001b[39m=\u001b[39m []\n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m document[\u001b[39m'\u001b[39m\u001b[39mpdf_json_files\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m; \u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m     filepath \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(covid_dirpath, filename)\n\u001b[0;32m     22\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_file_to_json(filepath)\n\u001b[0;32m     24\u001b[0m     \u001b[39m# Split each paragraph into multiple sentences first, in order to improve the performance of the word tokenizer\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'covid_dirpath' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "word_vectors = []\n",
        "ids = []\n",
        "\n",
        "for (words, doc_id) in Corpus(N_DOCUMENTS).plain_iter():\n",
        "    ids.append(doc_id)\n",
        "    word_vector = model.infer_vector(words)\n",
        "    word_vectors.append(word_vector)\n",
        "    if len(word_vectors) % 100 == 0:\n",
        "        print(f'Processed {len(word_vectors)} documents.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we've finished reading the articles, we can dismount the dataset in order to free up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "gather": {
          "logged": 1676793905367
        }
      },
      "outputs": [],
      "source": [
        "#mount.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "gather": {
          "logged": 1676793905599
        }
      },
      "outputs": [],
      "source": [
        "wv_df = pd.DataFrame(word_vectors, index=ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll join the DataFrame containing the numerical embeddings with the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "gather": {
          "logged": 1676793905820
        }
      },
      "outputs": [],
      "source": [
        "indexed_metadata = metadata_with_docs.set_index('cord_uid')\n",
        "metadata_with_embeddings = pd.concat([indexed_metadata.iloc[:N_DOCUMENTS], wv_df], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering documents\n",
        "\n",
        "We've determined the acceptable value for the clusters, so let's use Machine Learning to determine those clusters. We'll use the classic KMeans algorithm to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "gather": {
          "logged": 1676793909538
        }
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "at least one array or dtype is required",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[65], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m visualizer \u001b[39m=\u001b[39m KElbowVisualizer(KMeans(), k\u001b[39m=\u001b[39m(\u001b[39m3\u001b[39m,\u001b[39m20\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m visualizer\u001b[39m.\u001b[39;49mfit(wv_df)\n\u001b[0;32m      3\u001b[0m \u001b[39m# clusterer = KMeans(12 if visualizer.elbow_value_ > 12 else visualizer.elbow_value_)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# clusterer.fit(wv_df)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# clusters = clusterer.labels_\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\yellowbrick\\cluster\\elbow.py:339\u001b[0m, in \u001b[0;36mKElbowVisualizer.fit\u001b[1;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39m# Set the k value and fit the model\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39mset_params(n_clusters\u001b[39m=\u001b[39mk)\n\u001b[1;32m--> 339\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    341\u001b[0m \u001b[39m# Append the time and score to our plottable metrics\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_timers_\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start)\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1417\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m \n\u001b[0;32m   1392\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[39m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m-> 1417\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m   1418\u001b[0m     X,\n\u001b[0;32m   1419\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1420\u001b[0m     dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[0;32m   1421\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1422\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy_x,\n\u001b[0;32m   1423\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1424\u001b[0m )\n\u001b[0;32m   1426\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m   1428\u001b[0m random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\sklearn\\base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 546\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    547\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    548\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
            "File \u001b[1;32mc:\\Users\\photo\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:778\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    774\u001b[0m     pandas_requires_conversion \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(\n\u001b[0;32m    775\u001b[0m         _pandas_dtype_needs_early_conversion(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dtypes_orig\n\u001b[0;32m    776\u001b[0m     )\n\u001b[0;32m    777\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(dtype_iter, np\u001b[39m.\u001b[39mdtype) \u001b[39mfor\u001b[39;00m dtype_iter \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[1;32m--> 778\u001b[0m         dtype_orig \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mresult_type(\u001b[39m*\u001b[39;49mdtypes_orig)\n\u001b[0;32m    780\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(array, \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(array, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    781\u001b[0m     \u001b[39m# array is a pandas series\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     pandas_requires_conversion \u001b[39m=\u001b[39m _pandas_dtype_needs_early_conversion(array\u001b[39m.\u001b[39mdtype)\n",
            "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mresult_type\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: at least one array or dtype is required"
          ]
        }
      ],
      "source": [
        "visualizer = KElbowVisualizer(KMeans(), k=(3,20))\n",
        "visualizer.fit(wv_df)\n",
        "clusterer = KMeans(12 if visualizer.elbow_value_ > 12 else visualizer.elbow_value_)\n",
        "clusterer.fit(wv_df)\n",
        "clusters = clusterer.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll add each article's cluster as new column to our combined dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "gather": {
          "logged": 1676793909799
        }
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'clusters' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[49], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m metadata_with_clusters \u001b[39m=\u001b[39m metadata_with_embeddings\n\u001b[1;32m----> 2\u001b[0m metadata_with_clusters[\u001b[39m'\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m clusters\n\u001b[0;32m      3\u001b[0m metadata_with_clusters\n",
            "\u001b[1;31mNameError\u001b[0m: name 'clusters' is not defined"
          ]
        }
      ],
      "source": [
        "metadata_with_clusters = metadata_with_embeddings\n",
        "metadata_with_clusters['cluster'] = clusters\n",
        "metadata_with_clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now split our data into two datasets - a **training** one that will be used to train a Machine Learning model, able to determine the cluster that should be assigned to an article, and a **test** one that we'll use to test this classifier.\n",
        "\n",
        "We will allocate 80% of the articles to training the Machine Learning model, and the remaining 20% to testing it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676793910073
        }
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(metadata_with_clusters, train_size=0.8)\n",
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To speed up training, we'll ignore all columns except the word vectors calculated using Doc2Vec. For this reason, we will create a separate dataset just with the vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676793910280
        }
      },
      "outputs": [],
      "source": [
        "columns_to_ignore = ['sha', 'source_x', 'title', 'doi', 'pmcid', 'pubmed_id', 'license', 'abstract', 'publish_time', 'authors', 'journal', 'mag_id',\n",
        "                     'who_covidence_id', 'arxiv_id', 'pdf_json_files', 'pmc_json_files', 'url', 's2_id' ]\n",
        "train_data_vectors = train.drop(columns_to_ignore, axis=1)\n",
        "test_data_vectors = test.drop(columns_to_ignore, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register the training and testing datasets for AutoML availability\n",
        "\n",
        "We're registering the training and testing datasets with the Azure Machine Learning datastore to make them available inside Azure Machine Learning Studio and Automated ML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676793914243
        }
      },
      "outputs": [],
      "source": [
        "# Retrieve your ML workspace\n",
        "ws = Workspace.from_config()\n",
        "# Retrieve the workspace's default datastore\n",
        "datastore = ws.get_default_datastore()\n",
        "\n",
        "Dataset.Tabular.register_pandas_dataframe(train_data_vectors, datastore, 'COVID19Articles_Train')\n",
        "Dataset.Tabular.register_pandas_dataframe(test_data_vectors, datastore, 'COVID19Articles_Test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676794165844
        }
      },
      "outputs": [],
      "source": [
        "# The name of the compute instance\n",
        "compute_name = 'aml-compute-cpu'\n",
        "# The minimum and maximum number of nodes of the compute instance\n",
        "compute_min_nodes = 1\n",
        "# Setting the number of maximum nodes to a higher value will allow Automated ML to run more experiments in parallel, but will also inccrease your costs\n",
        "compute_max_nodes = 1\n",
        "\n",
        "vm_size = 'STANDARD_DS3_V2'\n",
        "\n",
        "# Check existing compute targets in the workspace for a compute with this name\n",
        "if compute_name in ws.compute_targets:\n",
        "    compute_target = ws.compute_targets[compute_name]\n",
        "    if compute_target and type(compute_target) is AmlCompute:\n",
        "        print(f'Found existing compute target: {compute_name}')    \n",
        "else:\n",
        "    print(f'A new compute target is needed: {compute_name}')\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
        "                                                                min_nodes = compute_min_nodes, \n",
        "                                                                max_nodes = compute_max_nodes)\n",
        "\n",
        "    # Create the cluster\n",
        "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
        "    \n",
        "    # Wait for provisioning to complete\n",
        "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676794166087
        }
      },
      "outputs": [],
      "source": [
        "# Retrieve the COVID19Articles_Train dataset from the workspace\n",
        "train_data = Dataset.get_by_name(ws, 'COVID19Articles_Train')\n",
        "\n",
        "\n",
        "\n",
        "# Configura Automated ML\n",
        "automl_config = AutoMLConfig(task = \"classification\",\n",
        "                             # Use weighted area under curve metric to evaluate the models\n",
        "                             primary_metric='AUC_weighted',\n",
        "                             \n",
        "                             # Use all columns except the ones we decided to ignore\n",
        "                             training_data = train_data,\n",
        "                             \n",
        "                             # The values we're trying to predict are in the `cluster` column\n",
        "                             label_column_name = 'cluster',\n",
        "                             \n",
        "                             # Evaluate the model with 5-fold cross validation\n",
        "                             n_cross_validations=5,\n",
        "                             \n",
        "                             # The experiment should be stopped after 15 minutes, to minimize cost\n",
        "                             experiment_timeout_hours=.25,\n",
        "                             #blocked_models=['XGBoostClassifier'],\n",
        "                             \n",
        "                             # Automated ML can try at most 1 models at the same time, this is also limited by the compute instance's maximum number of nodes\n",
        "                             max_concurrent_iterations=1,\n",
        "                             \n",
        "                             # An iteration should be stopped if it takes more than 5 minutes\n",
        "                             iteration_timeout_minutes=3,\n",
        "                             \n",
        "                             compute_target=compute_target,\n",
        "                             \n",
        "                             #The total number of different algorithm and parameter combinations to test during an automated ML experiment. If not specified, the default is 1000 iterations.\n",
        "                             iterations = 5\n",
        "                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676795096774
        }
      },
      "outputs": [],
      "source": [
        "# Use the `COVID19Articles_Train_Vectors` dataset\n",
        "exp = Experiment(ws, 'COVID19_Classification')\n",
        "run = exp.submit(automl_config, show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676795414027
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Retrieve the best performing run and its corresponding model from the aggregated Automated ML run\n",
        "best_run, best_model = run.get_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676795355958
        }
      },
      "outputs": [],
      "source": [
        "RunDetails(run).show()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "213055c14b8909d2b6639a9de47aeb416d9686ed66cda418a2d2d092e84a2dc1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
